{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CMTnQD2dH1yB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "OBXz5qxrH8Ei",
    "outputId": "10179189-83ea-49b5-cf32-10d157fa912b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define your folder structure\n",
    "data_dir = 'training_data'\n",
    "classes = ['cat', 'dog']\n",
    "\n",
    "# Load and preprocess audio data\n",
    "def load_and_preprocess_data(data_dir, classes, target_shape=(128, 128)):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith('.wav'):\n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "                audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "                # Perform preprocessing (e.g., convert to Mel spectrogram and resize)\n",
    "                mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "                mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)\n",
    "                data.append(mel_spectrogram)\n",
    "                labels.append(i)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "data, labels = load_and_preprocess_data(data_dir, classes)\n",
    "labels = to_categorical(labels, num_classes=len(classes))  # Convert labels to one-hot encoding\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a neural network model\n",
    "input_shape = X_train[0].shape\n",
    "input_layer = Input(shape=input_shape)\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output_layer = Dense(len(classes), activation='softmax')(x)\n",
    "model = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "PGkHLDk2H_IP"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "HOUlFgY0IB4S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10/10 [==============================] - 4s 296ms/step - loss: 18.5190 - accuracy: 0.7517 - val_loss: 14.1559 - val_accuracy: 0.8108\n",
      "Epoch 2/40\n",
      "10/10 [==============================] - 3s 255ms/step - loss: 0.6512 - accuracy: 0.9048 - val_loss: 6.6608 - val_accuracy: 0.8108\n",
      "Epoch 3/40\n",
      "10/10 [==============================] - 3s 254ms/step - loss: 0.1621 - accuracy: 0.9694 - val_loss: 4.9493 - val_accuracy: 0.7973\n",
      "Epoch 4/40\n",
      "10/10 [==============================] - 3s 264ms/step - loss: 0.1314 - accuracy: 0.9728 - val_loss: 5.0750 - val_accuracy: 0.7838\n",
      "Epoch 5/40\n",
      "10/10 [==============================] - 3s 255ms/step - loss: 0.0993 - accuracy: 0.9728 - val_loss: 5.8756 - val_accuracy: 0.7838\n",
      "Epoch 6/40\n",
      "10/10 [==============================] - 3s 264ms/step - loss: 0.0640 - accuracy: 0.9728 - val_loss: 6.5862 - val_accuracy: 0.7838\n",
      "Epoch 7/40\n",
      "10/10 [==============================] - 3s 264ms/step - loss: 0.0452 - accuracy: 0.9796 - val_loss: 7.3807 - val_accuracy: 0.7703\n",
      "Epoch 8/40\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.0344 - accuracy: 0.9796 - val_loss: 8.1834 - val_accuracy: 0.7703\n",
      "Epoch 9/40\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 0.0268 - accuracy: 0.9796 - val_loss: 8.7494 - val_accuracy: 0.7703\n",
      "Epoch 10/40\n",
      "10/10 [==============================] - 3s 259ms/step - loss: 0.0199 - accuracy: 0.9830 - val_loss: 9.2574 - val_accuracy: 0.8108\n",
      "Epoch 11/40\n",
      "10/10 [==============================] - 3s 273ms/step - loss: 0.0155 - accuracy: 0.9864 - val_loss: 9.6244 - val_accuracy: 0.8243\n",
      "Epoch 12/40\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.0135 - accuracy: 0.9864 - val_loss: 9.7684 - val_accuracy: 0.8108\n",
      "Epoch 13/40\n",
      "10/10 [==============================] - 3s 261ms/step - loss: 0.0126 - accuracy: 0.9864 - val_loss: 10.3969 - val_accuracy: 0.8243\n",
      "Epoch 14/40\n",
      "10/10 [==============================] - 3s 300ms/step - loss: 0.0116 - accuracy: 0.9864 - val_loss: 10.5378 - val_accuracy: 0.8243\n",
      "Epoch 15/40\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.0111 - accuracy: 0.9864 - val_loss: 10.5487 - val_accuracy: 0.8243\n",
      "Epoch 16/40\n",
      "10/10 [==============================] - 3s 288ms/step - loss: 0.0111 - accuracy: 0.9864 - val_loss: 10.2753 - val_accuracy: 0.8243\n",
      "Epoch 17/40\n",
      "10/10 [==============================] - 3s 292ms/step - loss: 0.0108 - accuracy: 0.9864 - val_loss: 10.7235 - val_accuracy: 0.8243\n",
      "Epoch 18/40\n",
      "10/10 [==============================] - 3s 295ms/step - loss: 0.0112 - accuracy: 0.9864 - val_loss: 9.9544 - val_accuracy: 0.8108\n",
      "Epoch 19/40\n",
      "10/10 [==============================] - 3s 282ms/step - loss: 0.0107 - accuracy: 0.9864 - val_loss: 10.3338 - val_accuracy: 0.8108\n",
      "Epoch 20/40\n",
      "10/10 [==============================] - 3s 286ms/step - loss: 0.0103 - accuracy: 0.9864 - val_loss: 10.7862 - val_accuracy: 0.8108\n",
      "Epoch 21/40\n",
      "10/10 [==============================] - 3s 293ms/step - loss: 0.0107 - accuracy: 0.9864 - val_loss: 9.6197 - val_accuracy: 0.8108\n",
      "Epoch 22/40\n",
      "10/10 [==============================] - 3s 264ms/step - loss: 0.0111 - accuracy: 0.9864 - val_loss: 9.0139 - val_accuracy: 0.8108\n",
      "Epoch 23/40\n",
      "10/10 [==============================] - 3s 283ms/step - loss: 0.0110 - accuracy: 0.9864 - val_loss: 9.4690 - val_accuracy: 0.8108\n",
      "Epoch 24/40\n",
      "10/10 [==============================] - 3s 273ms/step - loss: 0.0103 - accuracy: 0.9864 - val_loss: 10.0571 - val_accuracy: 0.8108\n",
      "Epoch 25/40\n",
      "10/10 [==============================] - 3s 272ms/step - loss: 0.0099 - accuracy: 0.9898 - val_loss: 10.5914 - val_accuracy: 0.8108\n",
      "Epoch 26/40\n",
      "10/10 [==============================] - 3s 277ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 10.9825 - val_accuracy: 0.8108\n",
      "Epoch 27/40\n",
      "10/10 [==============================] - 3s 268ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 11.1151 - val_accuracy: 0.8108\n",
      "Epoch 28/40\n",
      "10/10 [==============================] - 3s 257ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 12.0306 - val_accuracy: 0.7973\n",
      "Epoch 29/40\n",
      "10/10 [==============================] - 3s 285ms/step - loss: 0.0234 - accuracy: 0.9966 - val_loss: 9.7720 - val_accuracy: 0.7973\n",
      "Epoch 30/40\n",
      "10/10 [==============================] - 3s 264ms/step - loss: 0.0205 - accuracy: 0.9966 - val_loss: 7.8773 - val_accuracy: 0.8243\n",
      "Epoch 31/40\n",
      "10/10 [==============================] - 3s 260ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 7.8631 - val_accuracy: 0.8243\n",
      "Epoch 32/40\n",
      "10/10 [==============================] - 3s 310ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 8.1247 - val_accuracy: 0.8108\n",
      "Epoch 33/40\n",
      "10/10 [==============================] - 3s 259ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 8.4042 - val_accuracy: 0.8108\n",
      "Epoch 34/40\n",
      "10/10 [==============================] - 3s 261ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 8.5900 - val_accuracy: 0.8108\n",
      "Epoch 35/40\n",
      "10/10 [==============================] - 3s 261ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 8.8287 - val_accuracy: 0.8108\n",
      "Epoch 36/40\n",
      "10/10 [==============================] - 3s 263ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 9.0066 - val_accuracy: 0.8108\n",
      "Epoch 37/40\n",
      "10/10 [==============================] - 3s 264ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 9.1370 - val_accuracy: 0.8108\n",
      "Epoch 38/40\n",
      "10/10 [==============================] - 3s 257ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.2634 - val_accuracy: 0.8108\n",
      "Epoch 39/40\n",
      "10/10 [==============================] - 3s 263ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.3704 - val_accuracy: 0.8108\n",
      "Epoch 40/40\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 8.5559e-04 - accuracy: 1.0000 - val_loss: 9.4607 - val_accuracy: 0.8108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ff6da1b1d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "8zNwWmh6IE3t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 0s - loss: 9.4607 - accuracy: 0.8108 - 134ms/epoch - 45ms/step\n",
      "0.8108108043670654\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=2)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "nlh0IuxcIHTL"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('audio_classification_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4kyYUqo0IKG4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the saved model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_classification_model.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the target shape for input spectrograms\u001b[39;00m\n\u001b[0;32m      5\u001b[0m target_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = load_model('audio_classification_model.hdf5')\n",
    "\n",
    "# Define the target shape for input spectrograms\n",
    "target_shape = (128, 128)\n",
    "\n",
    "# Define your class labels\n",
    "classes = ['cat', 'dog']\n",
    "\n",
    "# Function to preprocess and classify an audio file\n",
    "def test_audio(file_path, model):\n",
    "    # Load and preprocess the audio file\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "    mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)\n",
    "    mel_spectrogram = tf.reshape(mel_spectrogram, (1,) + target_shape + (1,))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(mel_spectrogram)\n",
    "\n",
    "    # Get the class probabilities\n",
    "    class_probabilities = predictions[0]\n",
    "\n",
    "    # Get the predicted class index\n",
    "    predicted_class_index = np.argmax(class_probabilities)\n",
    "\n",
    "    return class_probabilities, predicted_class_index\n",
    "\n",
    "# Test an audio file\n",
    "test_audio_file = 'C:/Users/Admin/Desktop/training_data/Cat_effect.wav'\n",
    "class_probabilities, predicted_class_index = test_audio(test_audio_file, model)\n",
    "\n",
    "# Display results for all classes\n",
    "for i, class_label in enumerate(classes):\n",
    "    probability = class_probabilities[i]\n",
    "    print(f'Class: {class_label}, Probability: {probability:.4f}')\n",
    "\n",
    "# Calculate and display the predicted class and accuracy\n",
    "predicted_class = classes[predicted_class_index]\n",
    "accuracy = class_probabilities[predicted_class_index]\n",
    "print(f'The audio is classified as: {predicted_class}')\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
